{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 build model from block\n",
    "- tf.keras.Model类是tf.keras模块里提供的一个模型构造类，我们可以继承它来定义我们想要的模型。\n",
    "- 下面定义的MLP类重载了tf.keras.Model类的__init__函数和call函数。它们分别用于创建模型参数和定义前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()#Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dence1 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dence1(x)\n",
    "        output = self.dense2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的MLP类中无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的backward函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化MLP类得到模型变量net。\n",
    "\n",
    "下面的代码初始化net并传入输入数据X做一次前向计算。\n",
    "\n",
    "其中，net(X)将调用MLP类定义的call函数来完成前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform((2,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
       "array([[0.12335026, 0.71435344, 0.6334133 , 0.40966487, 0.13752842,\n",
       "        0.7016542 , 0.81597066, 0.61251533, 0.41511297, 0.2967198 ,\n",
       "        0.8559363 , 0.90746164, 0.773767  , 0.9487393 , 0.05601525,\n",
       "        0.04543793, 0.39873564, 0.23888302, 0.5986916 , 0.87822306],\n",
       "       [0.12450159, 0.27776515, 0.103459  , 0.3578192 , 0.5973673 ,\n",
       "        0.34271777, 0.40165794, 0.6890608 , 0.20921326, 0.1903454 ,\n",
       "        0.356449  , 0.695385  , 0.9641104 , 0.8011559 , 0.5936271 ,\n",
       "        0.9520819 , 0.773932  , 0.53147376, 0.01793122, 0.48219693]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.43565702,  0.02873291,  0.0973721 , -0.38374364, -0.25873524,\n",
       "        -0.3590687 ,  0.27803087,  0.37576792,  0.09905621,  0.08318231],\n",
       "       [-0.05725191, -0.08459843, -0.10647976, -0.12196989, -0.21263993,\n",
       "        -0.39308506,  0.13101135,  0.21821958,  0.04415824,  0.06083668]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sequential\n",
    "- tf.keras.Model类是一个通用的部件。事实上，Sequential类继承自tf.keras.Model类。当模型的前向计算为简单串联各个层的计算时，可以通过更加简单的方式定义模型。这正是Sequential类的目的：它提供add函数来逐一添加串联的Block子类实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。\n",
    "\n",
    "- 用Sequential类来实现前面描述的MLP类，并使用随机初始化的模型做一次前向计算。\n",
    "- Sequential类可以使模型构造更加简单，且不需要定义call函数，但直接继承tf.keras.Model类可以极大地拓展模型构造的灵活性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.08006182, -0.00380369, -0.11005731,  0.13930875,  0.04617419,\n",
       "         0.05314414,  0.30083013, -0.01032163, -0.10201715, -0.13135563],\n",
       "       [ 0.18882011,  0.06640825,  0.08350617, -0.02319756,  0.01685662,\n",
       "         0.00275064,  0.19363008,  0.09534745, -0.04183264,  0.06247529]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "])\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 build complex model\n",
    "- 构造一个稍微复杂点的网络FancyMLP。在这个网络中，我们通过constant函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用tensor的函数和Python的控制流，并多次调用相同的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.rand_weight = tf.constant(\n",
    "            tf.random.uniform((20,20)))\n",
    "        self.dense = tf.keras.layers.Dense(units=20, activation=tf.nn.relu)\n",
    "\n",
    "    def call(self, inputs):         \n",
    "        x = self.flatten(inputs)   \n",
    "        x = tf.nn.relu(tf.matmul(x, self.rand_weight) + 1)\n",
    "        x = self.dense(x)    \n",
    "        while tf.norm(x) > 1:\n",
    "            x /= 2\n",
    "        if tf.norm(x) < 0.8:\n",
    "            x *= 10\n",
    "        return tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个FancyMLP模型中，我们使用了常数权重rand_weight（注意它不是模型参数）、做了矩阵乘法操作（tf.matmul）并重复使用了相同的Dense层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.1951232>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FancyMLP()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为FancyMLP和Sequential类都是tf.keras.Model类的子类，所以我们可以嵌套调用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=25.283884>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential()\n",
    "        self.net.add(tf.keras.layers.Flatten())\n",
    "        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "        self.dense = tf.keras.layers.Dense(units=16, activation=tf.nn.relu)\n",
    "\n",
    "    \n",
    "    def call(self, inputs):         \n",
    "        return self.dense(self.net(inputs))\n",
    "\n",
    "net = tf.keras.Sequential()\n",
    "net.add(NestMLP())\n",
    "net.add(tf.keras.layers.Dense(20))\n",
    "net.add(FancyMLP())\n",
    "\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.模型参数的访问、初始化和共享\n",
    "- 本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.28309575,  0.33035207,  0.00477698,  0.38263905, -0.06752132,\n",
       "         0.18543263, -0.00471738,  0.14215837,  0.14404565,  0.14132789],\n",
       "       [-0.20838147,  0.26619714, -0.1374893 ,  0.35440785,  0.06613217,\n",
       "         0.07102975,  0.3096976 , -0.03204616,  0.08957964,  0.2130332 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "])\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 access model parameters\n",
    "- 对于使用Sequential类构造的神经网络，我们可以通过weights属性来访问网络任一层的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_16/kernel:0' shape=(20, 256) dtype=float32, numpy=\n",
       "array([[-0.01903628, -0.14045799,  0.06267899, ..., -0.03502336,\n",
       "         0.14657515, -0.13448143],\n",
       "       [ 0.13456693, -0.02639709,  0.01701823, ..., -0.04758588,\n",
       "         0.13559869, -0.01687135],\n",
       "       [-0.12671478, -0.0481895 ,  0.08373944, ..., -0.10435146,\n",
       "        -0.09566057,  0.14666396],\n",
       "       ...,\n",
       "       [ 0.12309813, -0.04828718, -0.04760803, ..., -0.14705625,\n",
       "         0.03321984,  0.04508564],\n",
       "       [ 0.04203133, -0.02572612,  0.10581619, ...,  0.05057153,\n",
       "        -0.05203209, -0.05922529],\n",
       "       [-0.10549372,  0.11880544, -0.1323233 , ...,  0.1088658 ,\n",
       "        -0.1132108 , -0.08223248]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2initialize params\n",
    "- 在下面的例子中，我们将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(\n",
    "            units=10,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.01),\n",
    "            bias_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "        self.d2 = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.ones_initializer(),\n",
    "            bias_initializer=tf.ones_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        output = self.d1(input)\n",
    "        output = self.d2(output)\n",
    "        return output\n",
    "\n",
    "net = Linear()\n",
    "net(x)\n",
    "net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 define initializer\n",
    "- 可以使用tf.keras.initializers类中的方法实现自定义初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_18/kernel:0' shape=(20, 64) dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init():\n",
    "    return tf.keras.initializers.Ones()\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, kernel_initializer=my_init()))\n",
    "\n",
    "Y = model(x)\n",
    "model.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层和后面章节中将要介绍的卷积层、池化层与循环层。虽然tf.keras提供了大量常用的层，但有时候我们依然希望自定义层。本节将介绍如何自定义一个层，从而可以被重复调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 custom layer without parameters\n",
    "CenteredLayer类通过继承tf.keras.layers.Layer类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了call函数里。这个层里不含模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs - tf.reduce_mean(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以实例化这个层，然后做前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([-2, -1,  0,  1,  2])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(np.array([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以用它来构造更复杂的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
       "array([[-0.18561336,  0.7980589 ,  0.5337747 , -0.86443174,  0.13248481,\n",
       "         0.32119542, -0.5011614 ,  0.5986253 ,  0.24122542, -0.22686513,\n",
       "         0.38301235,  0.11574471,  0.7210884 ,  0.28445512, -0.2951304 ,\n",
       "        -0.96856207,  0.8118604 , -0.42314735, -0.11699405,  0.2758397 ],\n",
       "       [-0.21926036, -0.08459637,  0.3569438 , -0.5331099 , -0.34114537,\n",
       "         0.4048779 , -0.92176265,  0.07205981, -0.42045605, -0.32289994,\n",
       "         0.8264247 ,  0.1391777 ,  0.6410753 , -0.5132173 , -0.10635653,\n",
       "        -0.37071216,  0.6091706 , -0.41690716, -0.38988742, -0.04487823]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential()\n",
    "net.add(tf.keras.layers.Flatten())\n",
    "net.add(tf.keras.layers.Dense(20))\n",
    "net.add(CenteredLayer())\n",
    "\n",
    "Y = net(x)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 custom layer with parameters\n",
    "- 还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):     # 这里 input_shape 是第一次运行call()时参数inputs的形状\n",
    "        self.w = self.add_weight(name='w',\n",
    "            shape=[input_shape[-1], self.units], initializer=tf.random_normal_initializer())\n",
    "        self.b = self.add_weight(name='b',\n",
    "            shape=[self.units], initializer=tf.zeros_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y_pred = tf.matmul(inputs, self.w) + self.b\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化MyDense类并访问它的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0662183 , -0.07223793, -0.02863855],\n",
       "        [ 0.05602537, -0.01299378,  0.02967717],\n",
       "        [ 0.07740184,  0.00111303, -0.02963178],\n",
       "        [ 0.00070688,  0.00184615,  0.00323411],\n",
       "        [-0.04707849, -0.0902397 , -0.04093401],\n",
       "        [-0.05739418,  0.06970806, -0.04940173],\n",
       "        [ 0.01465408,  0.09121215,  0.04880067],\n",
       "        [-0.00669081, -0.00186088, -0.10201043],\n",
       "        [ 0.03363297, -0.03407352,  0.0095716 ],\n",
       "        [-0.04044167, -0.09362209, -0.09052042],\n",
       "        [-0.00043173,  0.04064349,  0.04312221],\n",
       "        [ 0.00928534,  0.04488818, -0.1534695 ],\n",
       "        [ 0.02520572, -0.00927218,  0.06498551],\n",
       "        [ 0.00830077, -0.02744145,  0.05858869],\n",
       "        [ 0.01789406, -0.04866096, -0.00970302],\n",
       "        [ 0.00092342, -0.01169716, -0.04284624],\n",
       "        [-0.03877211,  0.01373667,  0.08019301],\n",
       "        [ 0.01347374, -0.03372454,  0.03266772],\n",
       "        [ 0.09045189,  0.06500303,  0.0539767 ],\n",
       "        [ 0.03045775, -0.06434947, -0.04761484]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = myDense(3)\n",
    "dense(x)\n",
    "dense.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以使用自定义层构造模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.00360968],\n",
       "       [0.0185812 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential()\n",
    "net.add(myDense(8))\n",
    "net.add(myDense(1))\n",
    "\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
